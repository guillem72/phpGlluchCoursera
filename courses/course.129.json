{
    "id": 129,
    "shortName": "hetero",
    "name": "Heterogeneous Parallel Programming",
    "language": "en",
    "shortDescription": "This course introduces concepts, languages, techniques, and patterns for programming heterogeneous, massively parallel processors. Its contents and structure have been significantly revised based on the experience gained from its initial offering in 2012. It covers heterogeneous computing architectures, data-parallel programming models, techniques for memory bandwidth management, and parallel algorithm patterns.",
    "aboutTheCourse": "<p>All computing\nsystems, from mobile to supercomputers, are becoming heterogeneous, massively\nparallel&nbsp;computers for higher power efficiency and&nbsp;computation\nthroughput. While the computing community is racing to build tools and\nlibraries to&nbsp;ease the use of these systems, effective and confident\nuse&nbsp;of these systems will always require knowledge about low-level\nprogramming&nbsp;in these systems. This course is designed for students to\nlearn the essence&nbsp;of low-level programming interfaces and how to use these\ninterfaces to achieve application goals. CUDA C, with its good balance between\nuser control and verboseness, will serve as the teaching vehicle for the first\nhalf of the course. Students will then extend their learning into closely\nrelated programming interfaces such as OpenCL, OpenACC, and C++AMP.<\/p>\n\n<p><span><br>\nThe course is unique in that it is application oriented and only introduces the\nnecessary&nbsp;underlying computer science and computer engineering knowledge for\nunderstanding.&nbsp;It covers the concept of data parallel execution models,\nmemory models for managing locality, tiling techniques for reducing bandwidth\nconsumption, parallel algorithm patterns,&nbsp;overlapping computation with\ncommunication, and a variety of\nheterogeneous parallel programming interfaces. The concepts learned in this\ncourse form a strong foundation for learning other types of parallel\nprogramming systems.<\/span><\/p>\n\n<br>",
    "targetAudience": 1,
    "courseSyllabus": "<ul><li><b>Week One:<\/b> Introduction to Heterogeneous\nComputing, Overview of CUDA C, and Kernel-Based Parallel Programming, with lab tour\nand programming assignment of vector addition in CUDA C.<\/li><li>&nbsp;<b>Week Two:<\/b> Memory Model for Locality, Tiling\nfor Conserving Memory Bandwidth, Handling Boundary Conditions, and Performance\nConsiderations, with&nbsp;programming assignment of simple matrix-matrix multiplication\nin CUDA C.<\/li><li><b>Week Three:<\/b> Parallel Convolution Pattern, with\nprogramming assignment of tiled matrix-matrix multiplication in CUDA C.<\/li><li><b>Week Four:<\/b> Parallel Scan Pattern, with\nprogramming&nbsp;assignment of parallel convolution in CUDA C<b>.<\/b><\/li><li><b>Week Five:<\/b> Parallel Histogram Pattern and\nAtomic Operations, with programming assignment of parallel scan in CUDA C.<\/li><li><b>Week Six:<\/b> Data Transfer and Task\nParallelism, with programming assignment of parallel histogram in CUDA C.<\/li><li><b>Week Seven:<\/b> Introduction to OpenCL,\nIntroduction to C++AMP, Introduction to OpenACC, with programming assignment of\nvector addition using streams in CUDA C.<\/li><li><b>Week Eight: <\/b>Course Summary,\nOther Related Programming Models â€“Thrust,&nbsp;Bolt, and CUDA FORTRAN, with\nprogramming assignment of simple matrix-matrix multiplication in choice of\nOpenCL, C++AMP, or OpenACC<b>.<\/b><\/li><li><b><\/b><b>Week Nine: <\/b>complete any\nremaining lab assignments, with optional, bonus programming assignments in choice\nof OpenCL, C++AMP, or OpenACC.\n\n<\/li>\n<\/ul>",
    "estimatedClassWorkload": "6-8 hours\/week",
    "recommendedBackground": "Programming experience in C\/C++.",
    "links": {

    }
}