{"id":749,"shortName":"artificialvision","name":"Discrete Inference and Learning in Artificial Vision","language":"en","shortDescription":"We will present the state of the art energy minimization algorithms that are used to perform inference in modern artificial vision models: that is, efficient methods for obtaining the most likely interpretation of a given visual input. We will also cover the popular max-margin framework for estimating the model parameters using inference.","aboutTheCourse":"<p>Artificial\nvision applications, such as object detection in natural images and automatic\nsegmentation of medical acquisitions, rely on models that interpret the visual\ninformation provided to a computer. The model provides a compromise between the\nsupport given by the observations and the prior domain knowledge. This course\nis concerned with the two computational problems that arise when using such models\nin practice.<\/p>\n\n<p><i>Inference\n(Energy Minimization):<\/i><\/p>\n\n<p>Given a visual observation (for example, an image or an MRI scan), we\nare interested in estimating its most likely interpretation (i.e. the location of all\nthe objects in the image, or the segments of the MRI scan) according to the\nmodel. While the problem cannot be solved optimally, we will describe state of the art approximate algorithms that provide very accurate solutions in\npractice. While the theoretical properties of the algorithms will be discussed\nbriefly, the main emphasis will be on their application.<\/p>\n\n<p><i>\u00a0Learning\n(Parameter Estimation):<\/i><br><\/p>\n\n<p>Given a set of training samples consisting of inputs and their desired\noutputs, (for example, images and the location of the objects, or MRI scans and\ntheir segmentations) we would like to estimate a model that is suited to the\ntask at hand. We will show how the problem of learning a model can be\nformulated as empirical risk minimization. Furthermore, we will present\nefficient algorithms for solving the corresponding optimization problem.<\/p>","targetAudience":1,"courseSyllabus":"<ul><li><b>Lecture\n1: Introduction to artificial vision with discrete graphical models<\/b>: In this lecture, the interdisciplinary nature of\ncomputational vision is briefly introduced along with its potential use in\ndifferent application domains. Subsequently, the concept of discrete modeling\nof artificial vision tasks is introduced from theoretical view point along with\nshort examples demonstrating the interest of such an approach in low, mid and\nhigh-level vision. Examples refer to blind image deconvolution, knowledge-based\nimage segmentation, optical flow, graph matching, 2d-to-3d view-point invariant\ndetection and modeling and grammar-driven image based reconstruction.<\/li><li><b>Lecture\n2: Reparameterization and dynamic programming<\/b>:<span> In this lecture, we provide a brief introduction to\nundirected graphical models. We also provide a formal definition of the problem\nof inference (specifically, energy minimization). We introduce the concept of\nreparameterization, which forms the building block of all the inference\nalgorithms discussed in the course. We describe a simple inference algorithm\nknown as dynamic programming, which consists of a series of reparameterization.\nWe show how dynamic programming can be used to perform exact inference on\nchains.<br>\n<\/span>\n\n<b> <\/b><\/li><li><b>Lecture\n3: <\/b><b><b>Maximum flow and minimum cut<\/b>:&nbsp; <\/b>In this lecture, we introduce the concept of functions on arcs of a directed graph. We\nfocus on a special function known as the flow function. Associated with this\nfunction is the combinatorial optimization problem of computing the maximum\nflow of a directed graph. We also introduce the concept of a cut in a directed\ngraph, and prove that the minimum cost cut is equivalent to the maximum flow.\nWe describe a simple algorithm for solving the maximum flow, or equivalent the\nminimum cut, problem.<\/li><li><b>Lecture\n4: <\/b><b>Minimum cut based inference<\/b>: In this lecture,<span> we show how the problem of inference for undirected\ngraphical models with two labels can be formulated as a minimum cut problem. We\ncharacterize the energy function that can be minimized optimally using the\nminimum cut problem. We show examples using the image segmentation and texture\nsynthesis problems, which can be formulated using two labels. We consider the\nmulti-label problem, and devise approximate algorithms for inference based on\nthe minimum cut algorithms. We show examples using the stereo reconstruction\nand the image denoising problems.<br>\n<\/span><\/li><li><b>Lecture\n5: Belief propagation: <\/b><span>In this lecture we present the basic concepts of\nmessage passing and belief propagation networks. The concept is initially\ndemonstrated using chains, extended to the case of trees and then eventually to\narbitrary graphs. The strengths and the limitations of such an optimization\nframework are presented. The image completion and texture synthesis problems\nare considered as examples to demonstrate the interest of such a family of\noptimization algorithms.<br>\n<\/span><\/li><li><b>Lecture\n6: Linear programing and duality:&nbsp; <\/b>In this lecture, discrete inference is addressed through concepts coming\nfrom linear programming relaxations. In particular, we explain how a\ngraph-optimization problem can be expressed as a linear programing one and then\nhow one can take benefit of the duality theorem to develop efficient\noptimization methods. The problem of optical flow and its deformable\nregistration variant in medical image analysis is considered as an example to\ndemonstrate the interest of such optimization algorithms.<\/li><li><b>Lecture\n7: Dual decomposition and higher order graphs: <\/b><span>In this lecture, we introduce the dual decomposition\nframework for the optimization of low rank and higher order graphical models.\nFirst, we demonstrate the concept of the method using a simple toy example and\nthen we extend to the most general optimization problem case. Three different\nexamples are considered in the context of higher order optimization, the\nproblem of linear mapping between images, the case of dense deformable graph\nmatching and the development of pose invariant object segmentation methods in\nthe context of medical imaging.<br>\n<\/span><\/li><li><b>Lecture 8: Parameter learning: <\/b>In this lecture, we introduce two frameworks for estimating the parameters\nof a graphical model using fully supervised training data. The first framework\nmaximizes the likelihood of the training data while regularizing the\nparameters. The second framework minimizes the empirical risk, as measured by a\nuser-defined loss function, while regularizing the parameters. We provide a\nbrief description of the algorithms required to solve the related optimization\nproblems. We show the results obtained on standard machine learning\ndatasets.\n<\/li><\/ul><br><br>","courseFormat":"<p>There\nwill be one lecture that is approximately one hour long each week. The lecture will\nbe broken down into a series of mini-videos that are roughly 10\nminutes in length. Each week, students will also be given a\nproblem set based on the topics covered in that week's lecture.<\/p>\n<br>","suggestedReadings":"<p>In\naddition to the lectures, some papers that are relevant to the topics\ncovered in this course will be provided. Those students who wish to\nlearn about other state of the art methods for inference and learning\nmight be interested in the following edited volume.<\/p>Andrew\nBlake, Pushmeet Kohli and Carsten Rother (editors).<i> <a target=\"_blank\" href=\"http:\/\/mitpress.mit.edu\/books\/markov-random-fields-vision-and-image-processing\">Advances\nin Markov Random Fields for Vision and Image Processing<\/a><\/i>. MIT\nPress, 2011.\n<br><br>Furthermore the reading of the following papers is highly recommended <i>(note: <b>all papers can be found through googlesearch on open access<\/b>)<\/i>:<br><ol><li>Chaohui Wang, Nikos Komodakis, Nikos Paragios: <a target=\"_blank\" href=\"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1077314213001343\">Markov Random Field \nmodeling, inference & learning in computer vision & image understanding: \nA survey<\/a>. Computer Vision and Image Understanding 117(11): 1610-1627 \n(2013)\u00a0<\/li><li>Yuri Boykov and Vladimir Kolmogorov: <a target=\"_blank\" href=\"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=1316848&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1316848\">An Experimental Comparison of \nMin-Cut\/Max-Flow Algorithms for Energy Minimization in Vision<\/a>, in IEEE \nTransactions on Pattern Analysis and Machine Intelligence, \n26(9):1124-1137 (2004).<\/li><li>Yuri Boykov and Olga Veksler:<a target=\"_blank\" href=\"http:\/\/www.csd.uwo.ca\/~yuri\/Papers\/chapter_04.pdf\"> Graph Cuts in Vision and Graphics: Theories and Applications<\/a>, in <a target=\"_blank\" href=\"http:\/\/www.springer.com\/computer\/image+processing\/book\/978-0-387-26371-7\">Handbook of Mathematical Models in Computer Vision<\/a>, edited by Nikos Paragios, Yunmei Chen and Olivier Faugeras. Springer, 2006. <br><\/li><li>Yuri Boykov, Olga Veksler and Ramin Zabih: <a target=\"_blank\" href=\"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=969114&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D969114\">Fast Approximate Energy \nMinimization via Graph Cuts<\/a>, in IEEE Transactions on Pattern Analysis \nand Machine Inteligence, 23(11): 1222-1239 (2001).<\/li><li>Vladimir Kolmogorov: <a target=\"_blank\" href=\"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=1677515&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F34%2F35279%2F01677515.pdf%3Farnumber%3D1677515\">Convergent Tree-reweighted Message Passing for \nEnergy Minimization<\/a>, in IEEE Transactions on Pattern Analysis and \nMachine Intelligence 28(10): 1568-1583 (2006).<\/li><li>Vladimir Kolmogorov and Ramin Zabih: <a target=\"_blank\" href=\"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=1262177&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1262177\">What Energy Functions can be \nMinimized via Graph Cuts?<\/a>, in IEEE Transactions on Pattern Analysis and \nMachine Inteligence, 26(2): 147-159 (2004).<\/li><li>Nikos Komodakis, Georgios Tziritas, Nikos Paragios: <a target=\"_blank\" href=\"http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1077314208000982\">Performance vs computational efficiency for optimizing single and dynamic MRFs: Setting the state of the art with primal-dual strategies, Computer Vision and Image Understanding 112(1), 14-29 (2008).\u00a0<\/a><\/li><li>Nikos Komodakis, Nikos Paragios, Georgios Tziritas: <a target=\"_blank\" href=\"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=5467090&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5467090\">MRF Energy Minimization and Beyond via Dual Decomposition<\/a>. IEEE Transactions on Pattern Analysis Machine Intelligence 33(3): 531-552 (2011)<\/li><li>Ben Taskar, Carlos Guestrin and Daphne Koller: <a target=\"_blank\" href=\"http:\/\/www.seas.upenn.edu\/~taskar\/pubs\/mmmn.pdf\">Max-Margin Markov \nNetworks<\/a>, in Proceedings of Advances in Neural Information Processing \nSystems: (2003). <br><\/li><li>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann and Yasmine \nAltun: <a target=\"_blank\" href=\"http:\/\/jmlr.org\/papers\/v6\/tsochantaridis05a.html\">Large Margin Methods for Structured and Interdependent Output \nVariables<\/a>, in Journal of Machine Learning Research, 6:1453-1484 (2005)\u00a0<\/li><li>Tomas Werner:<a target=\"_blank\" href=\"http:\/\/ieeexplore.ieee.org\/xpl\/login.jsp?tp=&arnumber=5128911&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5128911\"> Revisiting the Linear Programming Relaxation Approach to \nGibbs Energy Minimization and Weighted Constraint Satisfaction<\/a>, in IEEE \nTransactions on Pattern Analysis and Machine Intelligence 32(8): 1474-1488 (2010).<\/li><\/ol>","estimatedClassWorkload":"2-3 hours\/week","recommendedBackground":"<p>A\nbasic computer science course on algorithms is recommended, as it\nwould help in the understanding of the complexity of the problems and\ntheir solutions.<\/p>","links":{}}